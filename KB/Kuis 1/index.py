# -*- coding: utf-8 -*-
"""Kuis Kecerdasan Buatan #4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NS-OR2y6GDDelWchFfsWU7FByPDP39Ih

# Pendeteksi Komentar Positif dan Negatif

### Vincentius Johanes Lwie Jaya / 233408010
"""

!pip install gensim scikit-learn

"""```NOTED```

Setelah menjalankan perintah Install Library Langkah selanjutnya mulai ulang run time agar Fungsi dari Library dapat berjalan dengan baik
"""

import gensim, logging
from gensim.models.doc2vec import TaggedDocument
from gensim.models import Doc2Vec
import random
import re
import os
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.pipeline import make_pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
import multiprocessing

!curl -L -o /content/googlenewsvectors.zip https://www.kaggle.com/api/v1/datasets/download/adarshsng/googlenewsvectors

!unzip googlenewsvectors.zip

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

gmodel = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)

gmodel['cat']

gmodel['dog']

gmodel['spatula']

gmodel.similarity('cat', 'dog')

gmodel.similarity('cat','spatula')

def ekstrak_kata(terkirim):
  terkirim = terkirim.lower()
  terkirim = re.sub(r'<[^>]+>',' ',terkirim)# strip html tags
  terkirim = re.sub(r'(\w)\'(\w)', r'\1\2', terkirim)# remove apostrophes
  terkirim = re.sub(r'[^\w\s]',' ', terkirim)# remove punctuation
  terkirim = re.sub(r'\s+',' ', terkirim)# remove repeated spaces
  terkirim= terkirim.strip()
  return terkirim.split()

!wget -O aclImdb_v1.tar.gz https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
!tar -xzf aclImdb_v1.tar.gz

!wget https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz
!tar -xzf rt-polaritydata.tar.gz

!wget https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip
!unzip trainDevTestTrees_PTB.zip

# unsupervised training dato
unsup_sentences =[]
# link: http://ai.stonford.edu/~amaas/data/sentiment/, data mentah dari folder IMD8
for dirname in ["train/pos","train/neg","train/unsup","test/pos","test/neg"]:
  for fname in sorted(os.listdir("aclImdb/"+ dirname)):
    if fname[-4:]=='.txt':
      with open("aclImdb/"+ dirname +"/"+ fname, encoding ='UTF-8') as f:
        terkirim = f.read()
        words = ekstrak_kata(terkirim)
        unsup_sentences.append(TaggedDocument(words,[dirname + "/" + fname]))
# link: http://w.cs.cormeld.edu/people/pabo/movie-review-data
cornell_files = ["rt-polarity.pos", "rt-polarity.neg"]
for fname in cornell_files:
    if os.path.exists(fname):  # Check if file exists
        with open(fname, encoding='latin-1') as f:  # Cornell data uses latin-1 encoding
            for i, line in enumerate(f):
                words = ekstrak_kata(terkirim)
                unsup_sentences.append(TaggedDocument(words, ["cornell-" + fname + "-" + str(i)]))
# link: https://nip.Stanford.edu/sentinent/, data mentah dari folder Rotten Tomatoes
if os.path.exists("stanfordSentimentTreebank/original_rt_snippets.txt"):
    with open("stanfordSentimentTreebank/original_rt_snippets.txt", encoding='UTF-8') as f:
        for i, line in enumerate(f):
            words = ekstrak_kata(terkirim)
            unsup_sentences.append(TaggedDocument(words, ["rt-%d" % i]))

len (unsup_sentences)

unsup_sentences[0:1]

class PermuteSentences(object):
    def __init__(self, terkirim):
        self.terkirim = terkirim

    def __iter__(self):
        shuffled = list(self.terkirim)
        random.shuffle(shuffled)
        for terkirim in shuffled:
            yield terkirim

# Optimalisasi parameter
cores = multiprocessing.cpu_count()
permuter = PermuteSentences(unsup_sentences)

model = Doc2Vec(
    documents=permuter,
    dm=0,
    vector_size=100,
    window=5,
    min_count=2,
    epochs=20,
    hs=0,
    negative=5,
    workers=cores-1,
    alpha=0.025,
    min_alpha=0.0001,
    sample=1e-5,
    cores=4
)

# Persiapan vocabulary
model.build_vocab(permuter)

# Training model
model.train(
    permuter,
    total_examples=model.corpus_count,
    epochs=model.epochs,
    report_delay=60  # laporkan progress setiap 60 detik
)

model.save('reviews.d2v')
# menyimpan model dalam bentuk *.d2v,berfungst untuk digunakan kembali: model = Doc2Vec.Load(f'reviews.d2v')

model.infer_vector(ekstrak_kata("This place is not worth your time, let alone Vegas."))

cosine_similarity(
    [model.infer_vector(ekstrak_kata("This place is not worth your time, let alone Vegas."))],
    [model.infer_vector(ekstrak_kata("Service sucks."))])

cosine_similarity(
    [model.infer_vector(ekstrak_kata("Highly recommended."))],
    [model.infer_vector(ekstrak_kata("Service sucks."))])

!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip -O sentiment_labelled_sentences.zip
!unzip sentiment_labelled_sentences.zip

sentences = []
sentvecs = []
sentiments = []

base_path = "sentiment labelled sentences"

for fname in ["yelp", "amazon_cells", "imdb"]:
    file_path = os.path.join(base_path, f"{fname}_labelled.txt")

    if not os.path.exists(file_path):
        print(f"Warning: File not found - {file_path}")
        continue

    with open(file_path, encoding='UTF-8') as f:
        for i, line in enumerate(f):
            line_split = line.strip().split('\t')
            if len(line_split) >= 2:
                sentences.append(line_split[0])
                words = ekstrak_kata(line_split[0])
                sentvecs.append(model.infer_vector(words, epochs=10))
                sentiments.append(int(line_split[1]))

# Mengacak data
combined = list(zip(sentences, sentvecs, sentiments))
random.shuffle(combined)
sentences, sentvecs, sentiments = zip(*combined)

clf = KNeighborsClassifier(n_neighbors=9)
clfrf = RandomForestClassifier()

skor = cross_val_score(clf, sentvecs, sentiments, cv=5)
np.mean(skor), np.std(skor)

skor = cross_val_score(clfrf, sentvecs, sentiments, cv=5)
np.mean(skor),np.std(skor)

# kompile perbandingan kata dengan RadomForestClassifier
pipeline = make_pipeline(CountVectorizer(), TfidfTransformer(), RandomForestClassifier())

skor = cross_val_score(clfrf, sentvecs, sentiments,cv=5)
np.mean(skor), np.std(skor)